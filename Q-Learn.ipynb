{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathan/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random, math, gym\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython import display\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------- BRAIN ---------------------------\n",
    "\n",
    "class Brain:\n",
    "    def __init__(self, stateCnt, actionCnt):\n",
    "        self.stateCnt = stateCnt\n",
    "        self.actionCnt = actionCnt\n",
    "\n",
    "        self.model = self._createModel()\n",
    "        # self.model.load_weights(\"cartpole-basic.h5\")\n",
    "\n",
    "    def _createModel(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(output_dim=64, activation='relu', input_dim=stateCnt))\n",
    "        model.add(Dense(output_dim=actionCnt, activation='linear'))\n",
    "\n",
    "        opt = RMSprop(lr=0.00025)\n",
    "        model.compile(loss='mse', optimizer=opt)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def train(self, x, y, epoch=1, verbose=0):\n",
    "        self.model.fit(x, y, batch_size=64, nb_epoch=epoch, verbose=verbose)\n",
    "\n",
    "    def predict(self, s):\n",
    "        return self.model.predict(s)\n",
    "\n",
    "    def predictOne(self, s):\n",
    "        return self.predict(s.reshape(1, self.stateCnt)).flatten()\n",
    "\n",
    "#-------------------- MEMORY --------------------------\n",
    "class Memory:   # stored as ( s, a, r, s_ )\n",
    "    samples = []\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def add(self, sample):\n",
    "        self.samples.append(sample)        \n",
    "\n",
    "        if len(self.samples) > self.capacity:\n",
    "            self.samples.pop(0)\n",
    "\n",
    "    def sample(self, n):\n",
    "        n = min(n, len(self.samples))\n",
    "        return random.sample(self.samples, n)\n",
    "\n",
    "#-------------------- AGENT ---------------------------\n",
    "MEMORY_CAPACITY = 100000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "GAMMA = 0.99\n",
    "\n",
    "MAX_EPSILON = 1\n",
    "MIN_EPSILON = 0.01\n",
    "LAMBDA = 0.001      # speed of decay\n",
    "\n",
    "class Agent:\n",
    "    steps = 0\n",
    "    epsilon = MAX_EPSILON\n",
    "\n",
    "    def __init__(self, stateCnt, actionCnt):\n",
    "        self.stateCnt = stateCnt\n",
    "        self.actionCnt = actionCnt\n",
    "\n",
    "        self.brain = Brain(stateCnt, actionCnt)\n",
    "        self.memory = Memory(MEMORY_CAPACITY)\n",
    "        \n",
    "    def act(self, s):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.actionCnt-1)\n",
    "        else:\n",
    "            return np.argmax(self.brain.predictOne(s))\n",
    "\n",
    "    def observe(self, sample):  # in (s, a, r, s_) format\n",
    "        self.memory.add(sample)        \n",
    "\n",
    "        # slowly decrease Epsilon based on our eperience\n",
    "        self.steps += 1\n",
    "        self.epsilon = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * math.exp(-LAMBDA * self.steps)\n",
    "\n",
    "    def replay(self):    \n",
    "        batch = self.memory.sample(BATCH_SIZE)\n",
    "        batchLen = len(batch)\n",
    "\n",
    "        no_state = np.zeros(self.stateCnt)\n",
    "\n",
    "        states = np.array([ o[0] for o in batch ])\n",
    "        states_ = np.array([ (no_state if o[3] is None else o[3]) for o in batch ])\n",
    "\n",
    "        p = self.brain.predict(states)\n",
    "        p_ = self.brain.predict(states_)\n",
    "\n",
    "        x = np.zeros((batchLen, self.stateCnt))\n",
    "        y = np.zeros((batchLen, self.actionCnt))\n",
    "        \n",
    "        for i in range(batchLen):\n",
    "            o = batch[i]\n",
    "            s = o[0]; a = o[1]; r = o[2]; s_ = o[3]\n",
    "            \n",
    "            t = p[i]\n",
    "            if s_ is None:\n",
    "                t[a] = r\n",
    "            else:\n",
    "                t[a] = r + GAMMA * np.amax(p_[i])\n",
    "\n",
    "            x[i] = s\n",
    "            y[i] = t\n",
    "\n",
    "        self.brain.train(x, y)\n",
    "\n",
    "#-------------------- ENVIRONMENT ---------------------\n",
    "class Environment:\n",
    "    def __init__(self, problem):\n",
    "        self.problem = problem\n",
    "        self.env = gym.make(problem)\n",
    "\n",
    "    def run(self, agent):\n",
    "        s = self.env.reset()\n",
    "        R = 0 \n",
    "\n",
    "        while True:            \n",
    "            self.env.render()\n",
    "\n",
    "            a = agent.act(s)\n",
    "\n",
    "            s_, r, done, info = self.env.step(a)\n",
    "\n",
    "            if done: # terminal state\n",
    "                s_ = None\n",
    "\n",
    "            agent.observe( (s, a, r, s_) )\n",
    "            agent.replay()            \n",
    "\n",
    "            s = s_\n",
    "            R += r\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        print(\"Total reward:\", R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment2:\n",
    "    def __init__(self, grid_size):\n",
    "        self.grid_size = grid_size\n",
    "        \n",
    "    def run(self, agent):\n",
    "        self.reset()\n",
    "        s = self.state\n",
    "        R = 0 \n",
    "\n",
    "        while True:            \n",
    "            self.render()\n",
    "\n",
    "            a = agent.act(s)\n",
    "\n",
    "            s_, r, done = self.act(a)\n",
    "\n",
    "            if done: # terminal state\n",
    "                s_ = None\n",
    "\n",
    "            agent.observe( (s, a, r, s_) )\n",
    "            agent.replay()            \n",
    "\n",
    "            s = s_\n",
    "            R += r\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        print(\"Total reward:\", R)\n",
    "\n",
    "    def _update_state(self, action):\n",
    "        \"\"\"\n",
    "        Input: action and states\n",
    "        Ouput: new states and reward\n",
    "        \"\"\"\n",
    "        state = self.state\n",
    "        # 0 = left\n",
    "        # 1 = right\n",
    "        # 2 = down\n",
    "        # 3 = up\n",
    "        \n",
    "        fy, fx, py, px, t, d = state\n",
    "        \n",
    "        old_d = abs(fx - px) + abs(fy - py)\n",
    "        \n",
    "        if action == 0:\n",
    "            if px > 0:\n",
    "                px -= 1\n",
    "        if action == 1:\n",
    "            if px < self.grid_size-1:\n",
    "                px += 1\n",
    "        if action == 2:\n",
    "            if py > 0:\n",
    "                py-= 1\n",
    "        if action == 3:\n",
    "            if py < self.grid_size-1:\n",
    "                py += 1\n",
    "                \n",
    "        new_d = abs(fx - px) + abs(fy - py)\n",
    "        \n",
    "        out = np.array([fy, fx, py, px, t-1, old_d-new_d])\n",
    "        self.state = out\n",
    "\n",
    "    def _draw_state(self):\n",
    "        im_size = (self.grid_size,)*2\n",
    "        state = self.state\n",
    "        canvas = np.zeros(im_size)\n",
    "        canvas[state[0], state[1]] = 1  # draw fruit\n",
    "        canvas[state[2], state[3]] = 0.5  # draw basket\n",
    "        self.canvas = canvas\n",
    "        return canvas\n",
    "\n",
    "    def _get_reward(self):\n",
    "        fruit_y, fruit_x, player_y, player_x, t, d = self.state\n",
    "        \n",
    "        if fruit_x == player_x and fruit_y == player_y:\n",
    "            return 1\n",
    "        \n",
    "        if d == 1:\n",
    "            return 0.1\n",
    "        \n",
    "        if d == 0:\n",
    "            return -0.1\n",
    "        \n",
    "        if d == -1:\n",
    "            return -1\n",
    "\n",
    "\n",
    "    def _is_over(self):\n",
    "        fruit_y, fruit_x, player_y, player_x, t, d = self.state\n",
    "        \n",
    "        if t == 0:\n",
    "            return True\n",
    "        \n",
    "        if fruit_x == player_x and fruit_y == player_y:\n",
    "            return True\n",
    "    \n",
    "        return False\n",
    "\n",
    "    def observe(self):\n",
    "        canvas = self._draw_state()\n",
    "        return canvas.reshape((1, -1))\n",
    "    \n",
    "    def render(self):\n",
    "        self._draw_state()\n",
    "        plt.gca().cla()\n",
    "        plt.imshow(self.canvas)\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "\n",
    "    def act(self, action):\n",
    "        self._update_state(action)\n",
    "        reward = self._get_reward()\n",
    "        game_over = self._is_over()\n",
    "        return self.state, reward, game_over\n",
    "\n",
    "    def reset(self):\n",
    "        fruit_x = np.random.randint(0, self.grid_size-1)\n",
    "        fruit_y = np.random.randint(0, self.grid_size-1)\n",
    "        player_x = np.random.randint(0, self.grid_size-1)\n",
    "        player_y = np.random.randint(0, self.grid_size-1)\n",
    "        time = abs(fruit_x - player_x) + abs(fruit_y - player_y)\n",
    "        time *= 2\n",
    "        \n",
    "        while abs(fruit_x - player_x) + abs(fruit_y - player_y) < self.grid_size/2:\n",
    "            fruit_x = np.random.randint(0, self.grid_size-1)\n",
    "            fruit_y = np.random.randint(0, self.grid_size-1)\n",
    "            player_x = np.random.randint(0, self.grid_size-1)\n",
    "            player_y = np.random.randint(0, self.grid_size-1)\n",
    "            time = abs(fruit_x - player_x) + abs(fruit_y - player_y)\n",
    "            time *= 2\n",
    "            \n",
    "        self.state = np.asarray([fruit_y, fruit_x, player_y, player_x, time, 0])\n",
    "        self._draw_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff3c34460f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#-------------------- MAIN ----------------------------\n",
    "# PROBLEM = 'CartPole-v0'\n",
    "# env = Environment(PROBLEM)\n",
    "\n",
    "# stateCnt  = env.env.observation_space.shape[0]\n",
    "# actionCnt = env.env.action_space.n\n",
    "\n",
    "env = Environment2(10)\n",
    "stateCnt = 6\n",
    "actionCnt = 4\n",
    "\n",
    "agent = Agent(stateCnt, actionCnt)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        env.run(agent)\n",
    "finally:\n",
    "    agent.brain.model.save(\"cartpole-basic.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
